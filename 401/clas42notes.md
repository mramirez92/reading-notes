# 42 Ethics 

[Self Driving Car](https://www.freep.com/story/money/cars/2017/11/21/self-driving-cars-ethics/804805001/)

Self-driving cars are an incredible advancement in technology. You can program a computer to predict events, but life happens, and decisions have to be made in an instant. The ethical issue here is if some freak accident or unexpected situation where no matter what maneuver the car makes someone will get hurt, but who? How do program who to choose? It's an issue with harm. 

[The Code I'm Still Ashamed Of](https://www.freecodecamp.org/news/the-code-im-still-ashamed-of-e4c021dff55e)

We as developers can create powerful amazing things. We can reach so many people with our work. Because of that, we have to remind ourselves of the reach and influence our projects, products, and work will have on the consumer. Of course, we want to please our clients, but we also have to think ethically and see what we are creating from the consumer's point of view. This article was a reminder to think thoroughly about the code we write and its influence.

[The employee backlash over Google’s censored search engine for China, explained](https://www.vox.com/2018/8/17/17704526/google-dragonfly-censored-search-engine-china)
"1.1 Contribute to society and to human well-being, acknowledging that all people are stakeholders in computing." That's the very first line in the Code of Ethics in the ACM Code of Ethics and Professional Conduct. Google's Dragonfly project, that "could block internet users in China from seeing web pages that discuss human rights, peaceful protests, democracy and other topics blacklisted by China’s authoritarian government" would jeopordize the people of China well being. Creating and launching a tool like this would only open the door for more and more censorship and strengthen an authoritarian government. 

[Google Backtracks, Says Its AI Will Not Be Used for Weapons or Surveillance](https://gizmodo.com/in-reversal-google-says-its-ai-will-not-be-used-for-we-1826649327)

Autonomous weapons are not a thing of the distant future. Building them would bring science fiction to life, but where do we draw the line. This would bring harm to others and open the door for similar things. "A computing professional has an additional obligation to report any signs of system risks that might result in harm. If leaders do not act to curtail or mitigate such risks, it may be necessary to "blow the whistle" to reduce potential harm." This line in the code of ethics, the developers assigned to work on this software decided this was too much risk and blew the whistle. 

[A group of Microsoft employees are demanding the company ditch a US Army contract that they say makes them 'war profiteers'](https://web.archive.org/web/20200616232735/https://www.businessinsider.com/microsoft-employees-protest-contract-us-army-hololens-2019-2)

Technology will continue to evolve long after some of us are gone. World events and the current state will always dictate how we as people evolve. The employees cited in this article made a conscious decision that aligned with their values. Their view of what is ethical and unethical made them say "No" to their employer, one of the biggest companies, Microsoft. I don't fully agree with their decision, but I do believe in staying authentic and I applaud them for upholding their personal values. 


